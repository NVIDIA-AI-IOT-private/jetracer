{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apex Tracking - Train Model\n",
    "\n",
    "After following the data collection notebook, you should now have a dataset of images along with x, y coordinates of the apex.\n",
    "\n",
    "Now, in this notebook we'll train a neural network to predict these x, y coordinates given the image as input.  To do this, we'll minimize\n",
    "the mean squared error between the target point and output point.\n",
    "\n",
    "First, let's create the ``XYDataset`` class.  Make sure that it's pointed the the same directory that you set in the ``data_collection`` notebook.\n",
    "\n",
    "The XYDataset class we create also accepts a ``torchvision`` transform that is applied to the image.  We'll add ColorJitter, to modify the brightness, contrast, saturation and hue of the input image, as well as normalization.  We use the same normalization parameters that were used for the original models trained on ImageNet in the torchvision package.\n",
    "\n",
    "We've also added a parameter to the XYDataset called ``random_hflip``, which indicates if we should randomly flip the input image horizontally.  When we add this parameter, the ``x`` value of the target is negated so that the target falls on the same point in the mirrored image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "dataset = XYDataset('apex_dataset', transform=transform, random_hflip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a loader that uses this dataset and generates batches of shuffled samples to train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our neural network.  We'll start from a resnet18 model architecture, which is originally pretrained on ImageNet for classification.\n",
    "\n",
    "We replace the final layer (which originally has 1000 outputs for the 1000 imagenet classes), with a linear layer that has just 2 classes, for our x, y coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create the optimizer that we'll use to train the neural network.  We'll use the ``Adam`` optimizer with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the following cell to train the neural network for the number of epochs specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.445279\n",
      "1: 0.077040\n",
      "2: 0.043039\n",
      "3: 0.057193\n",
      "4: 0.053531\n",
      "5: 0.044843\n",
      "6: 0.040853\n",
      "7: 0.058367\n",
      "8: 0.052599\n",
      "9: 0.044711\n",
      "10: 0.021138\n",
      "11: 0.030148\n",
      "12: 0.020463\n",
      "13: 0.043705\n",
      "14: 0.036735\n",
      "15: 0.020968\n",
      "16: 0.016151\n",
      "17: 0.011030\n",
      "18: 0.013973\n",
      "19: 0.014218\n",
      "20: 0.014213\n",
      "21: 0.011286\n",
      "22: 0.004724\n",
      "23: 0.012168\n",
      "24: 0.008409\n",
      "25: 0.013980\n",
      "26: 0.018960\n",
      "27: 0.013675\n",
      "28: 0.010431\n",
      "29: 0.015584\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for image, xy in iter(loader):\n",
    "        \n",
    "        image = image.to(device)\n",
    "        xy = xy.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xy_out = model(image)\n",
    "        \n",
    "        loss = F.mse_loss(xy_out, xy)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        \n",
    "    epoch_loss /= len(loader)\n",
    "    \n",
    "    print('%d: %f' % (epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the model for use in the live demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'apex_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
